{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fcd54b1",
   "metadata": {},
   "source": [
    "# Project Description\n",
    "\n",
    "## I. Project Goal\n",
    "\n",
    "The primary goal of this project is to predict the percentage change in the price of Crude Oil futures two minutes following the release of the Energy Information Agency's (EIA) Weekly Petroleum Status Report (WPSR).\n",
    "\n",
    "## II. Data Overview\n",
    "\n",
    "This project utilizes several key datasets to achieve its objective:\n",
    "\n",
    "1.  **EIA WPSR Forecast Data (`InvestingcomEIA.csv`)**\n",
    "    *   This dataset comprises historical records of EIA WPSR releases (reported in EST). Key variables include:\n",
    "        *   **Release Date and Time:** The date and time (EST) of the EIA report release.\n",
    "        *   **Actual Change in Crude Oil Inventories:** The officially reported weekly change in U.S. commercial crude oil inventories, in millions of barrels (figures are suffixed with 'M').\n",
    "        *   **Market Forecasts:** Pre-release consensus forecasts for the change in crude oil inventories (in millions of barrels), sourced from a third-party provider.\n",
    "        *   **Previous Period's Actual:** The actual inventory change from the preceding reporting period.\n",
    "\n",
    "2.  **EIA WPSR Crude Oil Net Imports (`net_import.csv`)**\n",
    "    *   This dataset contains historical weekly U.S. crude oil net import volumes. Key variables are:\n",
    "        *   **Date:** The Friday marking the end of the week for which net imports were recorded.\n",
    "        *   **Net Import:** The average daily net import volume of crude oil for the week, in thousands of barrels per day.\n",
    "\n",
    "3.  **EIA WPSR Weekly U.S. Field Production of Crude Oil (`weekly_prod.csv`)**\n",
    "    *   This dataset provides historical weekly data on U.S. field production of crude oil. Key variables include:\n",
    "        *   **Date:** The Friday marking the end of the week for which production was recorded.\n",
    "        *   **Production:** The average daily U.S. field production of crude oil for the week, in thousands of barrels per day.\n",
    "\n",
    "4.  **Minute-Resolution WTI Crude Oil Price Data (`cl-1m.csv`)**\n",
    "    *   This dataset provides high-frequency, minute-by-minute price and volume data for WTI crude oil futures (data timestamped in GMT-6/CST). Key variables include:\n",
    "        *   **Date and Time:** The date and time (GMT-6/CST) of each minute bar.\n",
    "        *   **Open, High, Low, Close (OHLC) Prices:** The open, high, low, and closing prices for each minute interval.\n",
    "        *   **Volume:** The trading volume within each minute interval.\n",
    "\n",
    "**Data Sourcing Note:** All EIA datasets (`InvestingcomEIA.csv`, `net_import.csv`, `weekly_prod.csv`) are publicly available online. The minute-resolution WTI crude oil price data (`cl-1m.csv`) was obtained from a commercial online market data provider.\n",
    "\n",
    "## III. Data Structure and Instances\n",
    "\n",
    "*   **Instance Definition:** Each instance in the final dataset used for machine learning represents a single weekly EIA WPSR release event.\n",
    "*   **Total Instances:** The dataset comprises **731** such instances.\n",
    "*   **Instance Composition:** Each instance is a consolidation of:\n",
    "    *   OHLCV (Open, High, Low, Close, Volume) price data for WTI crude oil futures, covering 60 minutes before and 2 minutes after the WPSR release.\n",
    "    *   Relevant statistics from the corresponding WPSR report, such as:\n",
    "        *   Change in crude oil inventories (supply)\n",
    "        *   Net imports\n",
    "        *   Production figures.\n",
    "\n",
    "## IV. Target Variable\n",
    "\n",
    "The target variable we aim to predict is the **percentage change in the price of WTI crude oil futures, calculated two minutes after the WPSR release time relative to the price at release time.**\n",
    "\n",
    "## V. Features for Prediction\n",
    "\n",
    "The features used to predict the target variable include:\n",
    "\n",
    "*   **Pre-Release Price Data:** Sixty minutes of minute-by-minute OHLC price data for WTI crude oil futures immediately preceding the WPSR release.\n",
    "*   **WPSR Report Statistics:**\n",
    "    *   Actual change in crude oil inventories (supply)\n",
    "    *   Net imports\n",
    "    *   Production figures\n",
    "*   **Release Time Price:** The exact price of WTI crude oil futures at the moment of the WPSR release."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f719c5",
   "metadata": {},
   "source": [
    "Problem Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dba9fbb",
   "metadata": {},
   "source": [
    "# Problem Setup\n",
    "\n",
    "### A. Data Cleaning/Pre-processing\n",
    "\n",
    "Prior to model development, the raw data from each source required several cleaning and pre-processing steps to address inconsistencies and prepare it for analysis:\n",
    "\n",
    "*   **Timezone Standardization:**\n",
    "    *   The datetime information in the EIA WPSR forecast data (reported in EST) and the Crude Oil Futures price data (reported in CST) were from different timezones. These were standardized to a single, consistent timezone (UTC) to ensure accurate temporal alignment of events.\n",
    "\n",
    "*   **Handling Missing Price Data (Zero-Volume Minutes):**\n",
    "    *   The Crude Oil Futures price data did not explicitly record minutes with zero trading activity. To create a continuous time series, these missing minute intervals were identified and imputed using forward-fill from the last known price.\n",
    "\n",
    "*   **Alignment of WPSR Data Indices:**\n",
    "    *   When integrating the various EIA WPSR datasets, an indexing discrepancy was identified. The WPSR supply data (from `InvestingcomEIA.csv`) was indexed by the report's release date. In contrast, the net import (`net_import.csv`) and production (`weekly_prod.csv`) datasets were indexed by the final Friday of the week to which the data pertained. These datasets were carefully aligned based on the relevant reporting week to ensure that all WPSR statistics corresponded to the correct release event.\n",
    "\n",
    "### B. Investigating Market Reaction to WPSR Release\n",
    "\n",
    "Our initial approach for this project was driven by the fundamental hypothesis that the information contained within the EIA's Weekly Petroleum Status Report (WPSR) is rapidly absorbed by the market upon its release. We posited that the crude oil market subsequently self-corrects its price to reflect this new information. Consequently, our first step was to investigate the speed of this information absorption and to determine if the report induced statistically significant changes in price behavior.\n",
    "To validate our hypothesis, we conducted the following exploratory analyses:\n",
    "\n",
    "1.  **Trading Volume Analysis:**\n",
    "    *   We first examined trading volume around the WPSR release time. As illustrated in the plot, there is a clear and significant spike in trading volume precisely at the time of the report's release. This observation strongly suggests that the market is indeed reacting to the information and actively trading based on it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02037ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this cell to produce Trading Volume Plot\n",
    "%run price_window_preprocessing.py\n",
    "price_window_60min.set_index('Datetime_CST', inplace=True)\n",
    "price_window_60min.sort_index(inplace=True)\n",
    "average_volume_by_time = price_window_60min.groupby('Time_to_Release_Minutes')['Volume'].mean().reset_index()\n",
    "\n",
    "fig_volume_scatter_release = go.Figure(data=[go.Scatter(\n",
    "    x=average_volume_by_time['Time_to_Release_Minutes'],\n",
    "    y=average_volume_by_time['Volume'],\n",
    "    mode='markers',\n",
    "    marker=dict(size=8),\n",
    "    text=average_volume_by_time['Volume'],\n",
    "    hovertemplate=\"Time to Release: %{x:.0f} minutes<br>Average Volume: %{y:.0f}<extra></extra>\"\n",
    ")])\n",
    "\n",
    "fig_volume_scatter_release.update_layout(\n",
    "    title='Average Trading Volume Around EIA WPSR Release',\n",
    "    xaxis_title='Minutes Relative to EIA Report Release',\n",
    "    yaxis_title='Average Volume',\n",
    "    xaxis=dict(\n",
    "        tickvals=[-60, -45, -30, -15, 0, 15, 30, 45, 60],\n",
    "        ticktext=['-60', '-45', '-30', '-15', 'Release', '+15', '+30', '+45', '+60']\n",
    "    ),\n",
    "    hovermode=\"closest\"\n",
    ")\n",
    "\n",
    "fig_volume_scatter_release.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60108479",
   "metadata": {},
   "source": [
    "\n",
    "2.  **Price Behavior Analysis:**\n",
    "    *   Next, we investigated changes in price behavior surrounding the release. This was approached in two ways:\n",
    "\n",
    "        *   **Visual Inspection via Heatmaps:**\n",
    "            We generated two heatmaps to compare minute-by-minute percentage price changes. In these heatmaps, each square's hue represents the frequency of specific price change magnitudes. One heatmap depicts price changes on days with a WPSR release, while the other shows price changes on non-release days. A visual comparison clearly indicates a distinct shift in price behavior at the time of the release, which gradually reverts to its typical pattern afterward.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81cff2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this cell to produce the two heatmaps\n",
    "time_intervals_release = sorted(price_window_60min['Time_to_Release_Minutes'].unique())\n",
    "price_change_bins = np.linspace(-1, 1, num=41)\n",
    "price_change_labels = [f'{bin_val:.2f}%' for bin_val in price_change_bins]\n",
    "\n",
    "price_window_60min['Price_Change_Bin'] = pd.cut(price_window_60min['Percent_Change'], bins=price_change_bins, labels=price_change_labels[:-1], include_lowest=True)\n",
    "\n",
    "\n",
    "heatmap_data_release = price_window_60min.groupby(['Price_Change_Bin', 'Time_to_Release_Minutes'], observed=False).size().unstack(fill_value=0)\n",
    "\n",
    "heatmap_data_release = heatmap_data_release.reindex(columns=time_intervals_release, fill_value=0)\n",
    "heatmap_data_release = heatmap_data_release.reindex(index=price_change_labels[:-1], fill_value=0)\n",
    "\n",
    "fig_heatmap_release = go.Figure(data=go.Heatmap(\n",
    "    z=heatmap_data_release.values,\n",
    "    x=heatmap_data_release.columns,\n",
    "    y=heatmap_data_release.index,\n",
    "    colorscale='Viridis',\n",
    "    colorbar=dict(title='Frequency')\n",
    "))\n",
    "\n",
    "fig_heatmap_release.update_layout(\n",
    "    title='Frequency of Percentage Price Changes Around EIA WPSR Release',\n",
    "    xaxis_title='Time Relative to Release (Minutes)',\n",
    "    yaxis_title='Percentage Price Change Bins',\n",
    "    yaxis=dict(autorange=\"reversed\"),\n",
    "    xaxis=dict(tickvals=time_intervals_release[::5], ticktext=[int(x) for x in time_intervals_release[::5]])\n",
    ")\n",
    "\n",
    "\n",
    "eia_release_dates = icom_eia_forecasts.reset_index()['Release_Datetime_CST'].dt.date\n",
    "min_wti_dates = min_WTI.reset_index()['Datetime_CST'].dt.date\n",
    "mask_non_eia_days = ~min_wti_dates.isin(eia_release_dates)\n",
    "\n",
    "min_WTI_no_eia_days = min_WTI.reset_index()[mask_non_eia_days].copy()\n",
    "\n",
    "wti_with_eia_releases = pd.merge_asof(\n",
    "    left=min_WTI_no_eia_days.reset_index(),\n",
    "    right=icom_eia_forecasts.reset_index(),\n",
    "    left_on='Datetime_CST',\n",
    "    right_on='Release_Datetime_CST',\n",
    "    direction='nearest',\n",
    "    tolerance=pd.Timedelta('2D')\n",
    ")\n",
    "\n",
    "wti_with_eia_releases = wti_with_eia_releases.set_index('Datetime_CST')\n",
    "\n",
    "wti_with_eia_releases.dropna(subset=['Release_Datetime_CST'], inplace=True)\n",
    "\n",
    "\n",
    "def time_difference(time1, time2):\n",
    "    \"\"\"\n",
    "    Calculates the time difference in seconds between two datetime.time objects.\n",
    "\n",
    "    Args:\n",
    "        time1: The first datetime.time object.\n",
    "        time2: The second datetime.time object.\n",
    "\n",
    "    Returns:\n",
    "        The time difference in minutes as a float.\n",
    "    \"\"\"\n",
    "    dummy_date = datetime.date(1, 1, 1)\n",
    "    datetime1 = datetime.datetime.combine(dummy_date, time1)\n",
    "    datetime2 = datetime.datetime.combine(dummy_date, time2)\n",
    "\n",
    "    time_delta = datetime2 - datetime1\n",
    "\n",
    "    return time_delta.total_seconds() / 60\n",
    "\n",
    "time_diff = [time_difference(t1, t2) for t1, t2 in zip(wti_with_eia_releases.reset_index()['Release_Datetime_CST'].dt.time, wti_with_eia_releases.reset_index()['Datetime_CST'].dt.time)]\n",
    "\n",
    "time_diff_series = pd.Series(time_diff)\n",
    "wti_with_eia_releases.reset_index(inplace=True)\n",
    "wti_with_eia_releases['Time_till_Release'] = (time_diff_series)\n",
    "wti_with_eia_releases.set_index('Datetime_CST', inplace=True)\n",
    "\n",
    "non_release_price_windows_60min = wti_with_eia_releases.copy().loc[wti_with_eia_releases['Time_till_Release'].abs() <= 60.0]\n",
    "non_release_price_windows_60min.reset_index(inplace=True)\n",
    "non_release_price_windows_60min.drop(columns=['Datetime_CST', 'Open', 'High', 'Low', 'Close', 'Release_Datetime_CST', 'Release_Datetime_EST'], inplace=True)\n",
    "\n",
    "time_intervals_non_release = sorted(non_release_price_windows_60min['Time_till_Release'].unique())\n",
    "\n",
    "non_release_price_windows_60min['Price_Change_Bin'] = pd.cut(non_release_price_windows_60min['Percent_Change'], bins=price_change_bins, labels=price_change_labels[:-1], include_lowest=True)\n",
    "\n",
    "\n",
    "heatmap_data_non_release = non_release_price_windows_60min.groupby(['Price_Change_Bin', 'Time_till_Release'], observed=False).size().unstack(fill_value=0)\n",
    "\n",
    "heatmap_data_non_release = heatmap_data_non_release.reindex(columns=time_intervals_non_release, fill_value=0)\n",
    "heatmap_data_non_release = heatmap_data_non_release.reindex(index=price_change_labels[:-1], fill_value=0)\n",
    "\n",
    "heatmap_non_release_values = heatmap_data_non_release.values\n",
    "\n",
    "min_val = np.min(heatmap_non_release_values)\n",
    "max_val = np.max(heatmap_non_release_values)\n",
    "\n",
    "if max_val > min_val:\n",
    "    heatmap_non_release_values_scaled_linear = (heatmap_non_release_values - min_val) / (max_val - min_val) * 100.0\n",
    "else:\n",
    "    heatmap_non_release_values_scaled_linear = np.zeros_like(heatmap_non_release_values)\n",
    "\n",
    "fig_heatmap_non_release = go.Figure(data=go.Heatmap(\n",
    "    z=heatmap_non_release_values_scaled_linear,\n",
    "    x=heatmap_data_non_release.columns,\n",
    "    y=heatmap_data_non_release.index,\n",
    "    colorscale='Viridis',\n",
    "    colorbar=dict(title='Frequency')\n",
    "))\n",
    "\n",
    "fig_heatmap_non_release.update_layout(\n",
    "    title='Frequency of Percentage Price Changes Around EIA WPSR Release On Non-Release Days',\n",
    "    xaxis_title='Time Relative to Release (Minutes)',\n",
    "    yaxis_title='Percentage Price Change Bins',\n",
    "    xaxis=dict(tickvals=time_intervals_non_release[::5], ticktext=[int(x) for x in time_intervals_non_release[::5]])\n",
    ")\n",
    "\n",
    "fig_heatmap_release.show()\n",
    "fig_heatmap_non_release.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfbe46d",
   "metadata": {},
   "source": [
    "*   <!-- for indent-->\n",
    "    *   **Statistical Verification via Kolmogorov-Smirnov Test:**\n",
    "                To statistically confirm the observed changes in price behavior, we performed a two-sample Kolmogorov-Smirnov (KS) test. This test compared the distributions of minute-by-minute price changes immediately following the WPSR release on \"release days\" versus comparable times on \"non-release days.\" The results, indicate that the price distributions/behavior during the first two minutes post-release are statistically significantly different from those on non-release days and from subsequent minutes on release days.\n",
    "            *(Note: For this test, a p-value threshold of 0.1 was chosen instead of the more conventional 0.05 due to the limited number of WPSR release events available for comparison.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e23beb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this cell to produce the KS Test polot\n",
    "from scipy import stats\n",
    "price_change_bins_labels = heatmap_data_non_release.index\n",
    "price_change_bins_midpoints = []\n",
    "for label in price_change_bins_labels:\n",
    "    lower_bound_str = label.split('%')[0]\n",
    "    midpoint = float(lower_bound_str) / 100.0\n",
    "    price_change_bins_midpoints.append(midpoint)\n",
    "price_change_bins_midpoints = np.array(price_change_bins_midpoints)\n",
    "\n",
    "minutes_to_test =pd.Series([float(m) for m in range(0, 16)])\n",
    "\n",
    "ks_results = []\n",
    "\n",
    "for minute in minutes_to_test:\n",
    "    if minute not in heatmap_data_non_release.columns or minute not in heatmap_data_release.columns:\n",
    "        print(f\"Minute {minute} not found in both heatmaps. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    sample_non_release = []\n",
    "    frequencies_non_release = heatmap_data_non_release[minute]\n",
    "    for i, freq in enumerate(frequencies_non_release):\n",
    "        sample_non_release.extend([price_change_bins_midpoints[i]] * freq)\n",
    "    sample_non_release = np.array(sample_non_release)\n",
    "\n",
    "    sample_release = []\n",
    "    frequencies_release = heatmap_data_release[minute]\n",
    "    for i, freq in enumerate(frequencies_release):\n",
    "        sample_release.extend([price_change_bins_midpoints[i]] * freq)\n",
    "    sample_release = np.array(sample_release)\n",
    "\n",
    "    if sample_non_release.size > 0 and sample_release.size > 0:\n",
    "        ks_statistic, p_value = stats.ks_2samp(sample_non_release, sample_release)\n",
    "        ks_results.append({\n",
    "            'Minute': float(minute),\n",
    "            'KS Statistic': ks_statistic,\n",
    "            'P-value': p_value\n",
    "        })\n",
    "    else:\n",
    "        ks_results.append({\n",
    "            'Minute': float(minute),\n",
    "            'KS Statistic': np.nan,\n",
    "            'P-value': np.nan,\n",
    "            'Warning': 'One or both samples are empty, KS test not performed.'\n",
    "        })\n",
    "\n",
    "ks_results_df = pd.DataFrame(ks_results)\n",
    "\n",
    "def create_ks_test_p_value_plot(ks_results_df):\n",
    "    \"\"\"\n",
    "    Generates a line plot of KS test P-values for price change distributions.\n",
    "\n",
    "    Args:\n",
    "        ks_results_df: DataFrame containing 'Minute' and 'P-value' columns.\n",
    "\n",
    "    Returns:\n",
    "        matplotlib.figure.Figure: The generated plot figure.\n",
    "    \"\"\"\n",
    "    line_plot_p_value_KS_test = plt.figure(figsize=(10, 6))\n",
    "    plt.plot(ks_results_df['Minute'], ks_results_df['P-value'], marker='o', linestyle='-')\n",
    "    plt.axhline(0.1, color='r', linestyle='--', label='Significance Level (0.1)')\n",
    "    plt.title('KS Test P-values for Price Change Distributions (Release vs. Non-Release Days)')\n",
    "    plt.xlabel('Minutes Relative to Release')\n",
    "    plt.ylabel('P-value')\n",
    "    plt.xticks(ks_results_df['Minute'])\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    return line_plot_p_value_KS_test\n",
    "\n",
    "line_plot_p_value_KS_test = create_ks_test_p_value_plot(ks_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb31265",
   "metadata": {},
   "source": [
    "# Algorithms\n",
    "\n",
    "Having established that the initial two minutes of trading activity post-WPSR release represent the most relevant period for price movement, our focus shifted to predicting the percentage price change within this window using machine learning techniques.\n",
    "\n",
    "Our group implemented and evaluated the following five regression algorithms:\n",
    "\n",
    "*   **Linear Regression** (Mia)\n",
    "*   **Random Forest Regression** (Quinn)\n",
    "*   **Extreme Gradient Boosting (XGBoost) Regression** (Evan)\n",
    "*   **Deep Neural Network (DNN)** (Terence)\n",
    "*   **K-Nearest Neighbors (KNN) Regression** (Samyak)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1360207",
   "metadata": {},
   "source": [
    "## Target Variable Justification: Percentage Price Change\n",
    "\n",
    "For this project, the target variable we aim to predict is the **percentage change in the WTI crude oil futures price, calculated two minutes after the WPSR release relative to the price at the moment of release.** This choice is deliberate and offers several key advantages for modeling financial time series, particularly in the context of event-driven price movements:\n",
    "\n",
    "1.  **Normalization and Comparability:**\n",
    "    *   Percentage change inherently normalizes the price movement by the prevailing price level. A $0.50 price move has a vastly different significance if crude oil is trading at $20/barrel versus $100/barrel. By using percentage change, we ensure that the target variable represents a relative magnitude of change, making predictions comparable across different time periods and varying absolute price levels. This is crucial as our dataset spans a considerable time, during which oil prices have fluctuated significantly.\n",
    "\n",
    "2.  **Stationarity Properties:**\n",
    "    *   Raw price series in finance are often non-stationary (e.g., they can exhibit trends, random walks, or unit roots). Non-stationary data can lead to spurious correlations and unreliable models. While not always perfectly stationary, log returns (which percentage changes closely approximate for small changes) tend to exhibit more stationary behavior than absolute price levels or even absolute price differences. This makes them more amenable to standard machine learning regression techniques.\n",
    "\n",
    "5.  **Addressing Heteroscedasticity (to some extent):**\n",
    "    *   Financial time series often exhibit volatility clustering (periods of high volatility followed by periods of low volatility). The variance of absolute price changes can be dependent on the price level (higher prices often mean larger absolute fluctuations). Percentage changes can help mitigate this effect, as the volatility of returns is often more stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72131b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre processing\n",
    "valid_releases = price_window_60min.loc[price_window_60min['Time_to_Release_Minutes'] == 2]\n",
    "price_window_60min = price_window_60min[price_window_60min['Date'].isin(valid_releases['Date'])]\n",
    "price_window_60min = price_window_60min[price_window_60min['Time_to_Release_Minutes'] <= 2]\n",
    "\n",
    "def pivot_market_data(df, x_minutes_before, cols_to_pivot=None):\n",
    "    \"\"\"\n",
    "    pivot into wide formate where minutes to release from [-x, 2] are kept as columns for each feature.\n",
    "    \"\"\"\n",
    "    if cols_to_pivot is None:\n",
    "        cols_to_pivot = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "\n",
    "    df_filtered = df[(df['Time_to_Release_Minutes'] <= 2) & (df['Time_to_Release_Minutes'] >= -x_minutes_before)].copy()\n",
    "\n",
    "    df_long = df_filtered.melt(\n",
    "        id_vars=['Datetime', 'Time_to_Release_Minutes', 'Release_Datetime'],\n",
    "        value_vars=cols_to_pivot,\n",
    "        var_name='Feature',\n",
    "        value_name='Value'\n",
    "    )\n",
    "    df_long['Feature_min'] = df_long['Feature'] + '_t' + df_long['Time_to_Release_Minutes'].astype(int).astype(str)\n",
    "\n",
    "    df_wide = df_long.pivot_table(\n",
    "        index='Release_Datetime',\n",
    "        columns='Feature_min',\n",
    "        values='Value'\n",
    "    ).reset_index()\n",
    "    cols_to_drop = [col for col in df_wide.columns if 't2' in col and col != 'Close_t2']\n",
    "    df_wide.drop(columns=cols_to_drop, inplace=True)\n",
    "    return df_wide\n",
    "\n",
    "df_wide = pivot_market_data(price_window_60min, x_minutes_before=60)\n",
    "\n",
    "def get_time_offset(col_name):\n",
    "    match = re.search(r't(-?\\d+)$', col_name)\n",
    "    if match: return int(match.group(1))\n",
    "    return float('inf')\n",
    "\n",
    "sorted_market_cols = sorted(df_wide.columns, key=get_time_offset)\n",
    "df_wide = df_wide[sorted_market_cols]\n",
    "df_wide.set_index('Release_Datetime', inplace=True)\n",
    "df_wide['Price_Change'] = df_wide['Close_t2'] - df_wide['Open_t0']\n",
    "#df_wide is now sorted\n",
    "\n",
    "df_wide.reset_index(inplace=True)\n",
    "df_wide['Release Date'] = df_wide['Release_Datetime'].dt.date\n",
    "df_wide = df_wide.drop(columns=['Release_Datetime']).set_index('Release Date')\n",
    "\n",
    "weekly_supply=pd.read_csv(\"InvestingcomEIA.csv\")\n",
    "weekly_supply['Release Date'] = pd.to_datetime(weekly_supply['Release Date'], format=\"%d-%b-%y\").dt.strftime(\"%Y-%m-%d\")\n",
    "weekly_supply.set_index('Release Date', inplace=True)\n",
    "\n",
    "\n",
    "weekly_prod=pd.read_csv(\"weekly_prod.csv\", header=2)\n",
    "\n",
    "\n",
    "weekly_import=pd.read_csv(\"net_import.csv\")\n",
    "\n",
    "df_wide = df_wide.sort_index()\n",
    "df_wide.reset_index(inplace=True)\n",
    "df_wide['Pct_Change'] = (df_wide['Close_t2'] - df_wide['Open_t0']) / df_wide['Open_t0'] * 100\n",
    "weekly_supply = weekly_supply.sort_index()\n",
    "weekly_supply.reset_index(inplace=True)\n",
    "\n",
    "df_wide['Release Date'] = pd.to_datetime(df_wide['Release Date'])\n",
    "weekly_supply['Release Date'] = pd.to_datetime(weekly_supply['Release Date'])\n",
    "price_supply = pd.merge(df_wide, weekly_supply, how='left', on='Release Date')\n",
    "\n",
    "weekly_prod['Date'] = pd.to_datetime(weekly_prod['Date'])\n",
    "weekly_import['Date'] = pd.to_datetime(weekly_import['Date'])\n",
    "\n",
    "prod_import = pd.merge(weekly_import, weekly_prod, how='left', on='Date').dropna()\n",
    "\n",
    "mapping = pd.read_csv('FXmappings.csv')\n",
    "split_data = mapping['Date'].str.split('(', expand=True)\n",
    "\n",
    "mapping['ReleaseDate_str'] = split_data[0].str.strip()\n",
    "\n",
    "mapping['end_date'] = split_data[1].str.rstrip(')').str.strip()\n",
    "\n",
    "mapping['ReleaseDate'] = pd.to_datetime(mapping['ReleaseDate_str'], format='%m/%d/%Y')\n",
    "mapping.drop(columns=['ReleaseDate_str', 'Date'], inplace=True)\n",
    "\n",
    "release_year = mapping['ReleaseDate'].dt.year\n",
    "\n",
    "full_end_date = mapping['end_date'] + ' ' + release_year.astype(str)\n",
    "\n",
    "mapping['end_date'] = pd.to_datetime(full_end_date, format='%b %d %Y')\n",
    "\n",
    "mapping['day_before'] = mapping['ReleaseDate'] - pd.Timedelta(days=1)\n",
    "\n",
    "dow = mapping['day_before'].dt.dayofweek\n",
    "\n",
    "days_to_subtract = (dow + 3) % 7\n",
    "\n",
    "mapping['end_date'] = mapping['day_before'] - pd.to_timedelta(days_to_subtract, unit='D')\n",
    "\n",
    "mapping.drop(columns=['day_before'], inplace=True)\n",
    "mapping['end_date'] = pd.to_datetime(mapping['end_date'])\n",
    "mapping = mapping.set_index('end_date').sort_index()\n",
    "mapping.reset_index(inplace=True)\n",
    "\n",
    "prod_import_mapped = pd.merge(mapping, prod_import, how='left', right_on='Date', left_on='end_date')\n",
    "prod_import_mapped.drop(columns=['end_date', 'Date'], inplace=True)\n",
    "\n",
    "full_data = pd.merge(price_supply, prod_import_mapped, how='left', left_on='Release Date', right_on='ReleaseDate')\n",
    "\n",
    "full_data.drop(columns=['Time', 'ReleaseDate'], inplace=True)\n",
    "full_data['Weekly Net Import'] = full_data['Weekly Net Import'] * 7\n",
    "full_data['Weekly Production'] = full_data['Weekly U.S. Field Production of Crude Oil  (Thousand Barrels per Day)'] * 7\n",
    "\n",
    "full_data = full_data.drop(columns=['Weekly U.S. Field Production of Crude Oil  (Thousand Barrels per Day)'])\n",
    "full_data = full_data.dropna()\n",
    "\n",
    "\n",
    "def convert_to_numeric(value):\n",
    "    multiplier = 1_000_000\n",
    "    numeric_part = value[:-1]\n",
    "\n",
    "    try:\n",
    "        return float(numeric_part) * multiplier\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "for col in ['Actual', 'Forecast', 'Previous']:\n",
    "\n",
    "    full_data[col] = full_data[col].apply(convert_to_numeric)\n",
    "\n",
    "for col in ['Weekly Net Import', 'Weekly Production']:\n",
    "    full_data[col] = full_data[col].apply(lambda x: x * 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecce6697",
   "metadata": {},
   "source": [
    "## Linear Regression (Mia Callahan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f682e257",
   "metadata": {},
   "source": [
    "### Design Justifications\n",
    "\n",
    "The Linear Regression model, including its Lasso-regularized variant, was implemented with several key design decisions aimed at establishing a robust baseline, understanding linear relationships, and managing potential overfitting given the high dimensionality of the pre-release market data.\n",
    "\n",
    "### 1. Choice of Linear Regression (and Lasso)\n",
    "\n",
    "*   **Decision**: Employ Linear Regression, subsequently augmented with Lasso (L1) regularization.\n",
    "*   **Justification**:\n",
    "    *   **Baseline Performance**: Linear Regression serves as a fundamental benchmark. Its performance provides a clear baseline against which more complex models (Random Forest, XGBoost, DNN) can be quantitatively compared. This helps in assessing whether the added complexity of other models translates into a significant improvement in predictive power for this specific financial prediction task.\n",
    "    *   **Interpretability**: Standard Linear Regression coefficients (especially after feature scaling) offer direct insights into the linear relationship (strength and direction) between each feature and the percentage price change. Lasso, by shrinking some coefficients to zero, further enhances interpretability by performing feature selection.\n",
    "    *   **Simplicity and Speed**: Linear models are computationally inexpensive and fast to train, making them ideal for initial data exploration and for establishing a quick performance reference.\n",
    "    *   **Identifying Linear Signals**: Despite the expectation of non-linearities in financial markets, this model helps to identify and quantify any strong, simple linear patterns that might be present in the feature set.\n",
    "\n",
    "### 2. Data Splitting Strategy\n",
    "\n",
    "*   **Decision**: Utilize a standard `train_test_split` methodology to create training, validation, and test sets (resulting in an 80% train, 10% validation, and 10% test split based on the code structure). `random_state=42` is used for reproducibility.\n",
    "*   **Justification**:\n",
    "    *   **Simplicity for Baseline Model**: For establishing a baseline, a randomized train-validation-test split is straightforward to implement and provides a quick assessment of general predictive capability on unseen data.\n",
    "    *   **Validation Set for Hyperparameter Tuning**: The creation of a separate validation set is crucial for tuning the `alpha` parameter of the Lasso regression without contaminating the final test set.\n",
    "\n",
    "### 3. Feature Scaling\n",
    "\n",
    "*   **Decision**: Apply `StandardScaler` to the input features before training the Linear Regression and Lasso models.\n",
    "*   **Justification**:\n",
    "    *   **Equal Feature Contribution**: Linear Regression models, and particularly regularized versions like Lasso, are sensitive to the scale of input features. Features with larger numerical ranges can disproportionately influence the model's coefficient fitting process. `StandardScaler` normalizes features by removing the mean and scaling to unit variance. This ensures all features are on a comparable scale, allowing the model to learn their importance based on their relationship with the target, not their magnitude.\n",
    "    *   **Improved Regularization Performance**: For Lasso regression, which applies an L1 penalty to the coefficients, feature scaling is important for fair penalization. Without scaling, features with larger values would naturally have larger coefficients, leading to them being penalized more heavily, irrespective of their true predictive power. Scaling helps the regularization process to be more effective and stable.\n",
    "    *   **Numerical Stability**: Can improve the numerical stability and convergence speed of the optimization algorithms used in fitting the models.\n",
    "\n",
    "### 4. Lasso Regression for Feature Selection and Regularization\n",
    "\n",
    "*   **Decision**: Implement Lasso (L1 Regularization) Regression and tune its `alpha` hyperparameter using the validation set.\n",
    "*   **Justification**:\n",
    "    *   **Addressing High Dimensionality & Preventing Overfitting**: The feature set includes 60 minutes of OHLCV data, leading to a large number of predictors (300 market features + Open_t0 + 4 fundamental features = 305 features before any interaction terms). Standard Linear Regression with many features is prone to overfitting the training data. Lasso adds an L1 penalty term to the loss function, which shrinks some less important coefficients towards zero, and can force some to be exactly zero. This penalizes model complexity and helps improve generalization to unseen data.\n",
    "    *   **Automatic Feature Selection**: By setting irrelevant feature coefficients to zero, Lasso performs an implicit form of feature selection. This is particularly useful when dealing with many potentially correlated or noisy features, as it helps in identifying a more parsimonious and interpretable model.\n",
    "    *   **Managing Multicollinearity**: While not its primary function, Lasso can handle multicollinearity by tending to select one feature from a group of highly correlated features and shrinking the coefficients of others.\n",
    "    *   **Alpha Hyperparameter Tuning**: The `alpha` parameter controls the strength of the L1 penalty. A small alpha leads to behavior similar to ordinary least squares, while a large alpha increases shrinkage, potentially leading to underfitting. The process of iterating through a range of `alpha` values and selecting the one that minimizes Mean Squared Error (MSE) on the validation set (as indicated by the code logic) is a sound method for finding an optimal trade-off between bias and variance.\n",
    "\n",
    "### 5. Evaluation Metric\n",
    "\n",
    "*   **Decision**: Use Root Mean Squared Error (RMSE) to evaluate model performance on the test set.\n",
    "*   **Justification**:\n",
    "    *   **Standard Regression Metric**: RMSE is a widely accepted and standard metric for evaluating the accuracy of regression models.\n",
    "    *   **Penalizes Larger Errors More Heavily**: Due to the squaring of errors before averaging, RMSE gives a higher weight to larger errors. In financial predictions, large, unexpected errors can be particularly detrimental, making RMSE a suitable metric if the goal is to minimize such occurrences.\n",
    "\n",
    "These design decisions collectively aim to build a well-regularized linear model that can serve as a solid baseline, provide insights into linear feature importance, and offer a fair comparison point for more advanced techniques applied in the project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c91da94",
   "metadata": {},
   "source": [
    "Running a linear regression acts as a baseline for our other models. We expect that the linear regression will not perform well, as the data likely has non-linear relationships. The bias-variance tradeoff likely will lead the linear regression to underfit the data as it is a very basic model. However, it will be interesting to see how it performs especially in comparison to the more complex models we will use later.\n",
    "\n",
    "First, we plot each variable against our outcome variable to determine if linear relationships between the variable can be estimated. (Plotted last 20 features for readability follow comments to see every feature plotted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de2b480",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run to see feature linear plots\n",
    "\n",
    "newy = df_wide['Pct_Change']\n",
    "feature_cols = [col for col in df_wide.columns if '_t2' not in col and 't_1' not in col and 't_0' not in col and col not in ['Close_t2', 'Release_Datetime', 'Date', 'Pct_Change', 'Price_Change']] + ['Open_t0']\n",
    "feature_cols = [col for col in feature_cols if col not in ['Unnamed: 0', 'Release Date']]\n",
    "ncols = 10\n",
    "nrows = 2 #-(-len(feature_cols) // ncols)\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(15, 2.5* nrows))\n",
    "axes = axes.flatten()\n",
    "X = df_wide[feature_cols]\n",
    "y = df_wide['Close_t2']\n",
    "\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings('ignore', category=ConvergenceWarning)\n",
    "\n",
    "for idx,col in enumerate(feature_cols[-20:]): # delete -20\n",
    "    ax = axes[idx]\n",
    "    xplot = X[col]\n",
    "    ax.plot(xplot,newy, \"b.\")\n",
    "    ax.set_xlabel(col, fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299f2fc5",
   "metadata": {},
   "source": [
    "Unsurprisingly, this leads to a much less linear relationship between the data. While this will lead to worse performance of my model, the intended use of prediciton models is to be able to profit off of stock market changes,so this is a more useful metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257e49d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, newy, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "#Scaling the data\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "linmodel = LinearRegression()\n",
    "linmodel.fit(X_train_scaled, y_train)\n",
    "linmodel.intercept_, linmodel.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0274c7be",
   "metadata": {},
   "source": [
    "Because there are so many features, we can see that the list of coefficients is very long. It is likely that the linear reegression will overfit because it will try to fit all of the features, even if they are not relevant. After seeing performance, we will reduce the number of features to see if this improves out of sample performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167b932f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = linmodel.predict(X_test_scaled)\n",
    "error = np.sqrt(np.mean((y_hat - y_test) ** 2))\n",
    "print(f\"RMSE on test set: {error:.2f}\")\n",
    "\n",
    "display(y_hat)\n",
    "display(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809d60b7",
   "metadata": {},
   "source": [
    "The RMSE of the model on the test set is .35, which is much better than I expected. This could change for a larger data sample. To try to prevent any overfitting from the large number of features, I will now use lasso to select the most useful features. First I will cross validate to find the ideal alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db128af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "mses = []\n",
    "best_mse = float('inf')\n",
    "best_alpha = None\n",
    "alphas = np.logspace(-10, 0, 100)\n",
    "for alpha in alphas:\n",
    "    lasso = Lasso(alpha=alpha,  random_state=42)\n",
    "    lasso.fit(X_train_scaled, y_tuning_full)\n",
    "    mse = mean_squared_error(y_val, lasso.predict(X_val_scaled))\n",
    "    mses.append(mse)\n",
    "    if mse < best_mse:\n",
    "        best_mse = mse\n",
    "        best_alpha = alpha\n",
    "print(f\"Best alpha: {best_alpha}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1f8f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_best = Lasso(alpha=best_alpha, random_state=42)\n",
    "lasso_best.fit(X_train_scaled, y_train)\n",
    "# see how it performs on out of sample data\n",
    "y_hat_lasso = lasso_best.predict(X_test_scaled)\n",
    "error_lasso = np.sqrt(np.mean((y_hat_lasso - y_test) ** 2))\n",
    "print(f\"RMSE on test set with Lasso: {error_lasso:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd683d6",
   "metadata": {},
   "source": [
    "The RMSE is higher than that using the full set of features. This indicates that most features are useful in improving out of sample prediciton. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7842a533",
   "metadata": {},
   "source": [
    "## Random Forest (Quinn Kelly)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ef39e0",
   "metadata": {},
   "source": [
    "\n",
    "### Why Random Forest Was Chosen for Implementation:\n",
    "\n",
    "Random Forest is a robust and widely used ensemble learning method, and its selection for predicting financial metrics (like price changes based on news sentiment) can be justified for several key reasons:\n",
    "\n",
    "1.  **Ability to Capture Non-Linear Relationships:**\n",
    "    *   Financial markets are notoriously complex, and the relationship between input features (e.g., news sentiment, technical indicators, economic data) and the target variable (e.g., price change) is rarely linear. Random Forest, being an ensemble of decision trees, is inherently capable of modeling complex, non-linear interactions between features without requiring explicit transformation or specification of these interactions beforehand.\n",
    "\n",
    "2.  **Robustness to Overfitting (Compared to Single Decision Trees):**\n",
    "    *   While individual decision trees can easily overfit the training data, Random Forest mitigates this risk through two main mechanisms:\n",
    "        *   **Bagging (Bootstrap Aggregating):** Each tree in the forest is trained on a random bootstrap sample of the training data.\n",
    "        *   **Feature Randomness:** At each split in a tree, only a random subset of features is considered.\n",
    "    *   This combination creates diverse trees, and averaging their predictions tends to reduce variance and lead to better generalization on unseen data. The use of `ccp_alpha` for pruning further helps control complexity and prevent overfitting.\n",
    "\n",
    "3.  **Good Performance with Less Hyperparameter Tuning (Relatively):**\n",
    "    *   Compared to some other advanced models (like Support Vector Machines or Neural Networks), Random Forests can often achieve good performance with less extensive hyperparameter tuning. They are often a strong \"out-of-the-box\" performer, making them a good choice for establishing a solid baseline or even as a primary model.\n",
    "\n",
    "4.  **Handling of Different Feature Types and Scales:**\n",
    "    *   Random Forests can handle numerical features directly without requiring extensive preprocessing like feature scaling (though scaling can sometimes still be beneficial for other parts of a pipeline or for consistency). This simplifies the preprocessing pipeline.\n",
    "\n",
    "5.  **Implicit Feature Importance Estimation:**\n",
    "    *   Random Forests provide a natural way to estimate the importance of each feature in making predictions (e.g., by measuring how much each feature contributes to reducing impurity across all trees). In a financial context, understanding which factors (e.g., sentiment from specific news categories, particular economic indicators) are most influential can be as valuable as the prediction itself.\n",
    "\n",
    "6.  **Robustness to Outliers and Noisy Data:**\n",
    "    *   Financial data is often noisy and can contain outliers. The ensemble nature of Random Forest, where predictions are averaged across many trees, makes the model more robust to the influence of individual outliers or noise points in the training data compared to models that might be heavily skewed by such points.\n",
    "\n",
    "7.  **Good Choice as a Benchmark or Comparison Model:**\n",
    "    *   Even if more complex models like Gradient Boosting (XGBoost, LightGBM) or Neural Networks are also being considered, Random Forest serves as an excellent benchmark. Its performance can provide a strong baseline against which other, potentially more complex or harder-to-tune, models can be compared. It represents a powerful, yet relatively understandable, class of non-linear models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccd59c0",
   "metadata": {},
   "source": [
    "### Design Justifications:\n",
    "\n",
    "### 1. Hyperparameters Grid and Tuning Strategy\n",
    "\n",
    "*   **Decision - `'n_estimators_list': [200, 400, 600]`, `'ccp_alpha_list': [10**-2, 10**-3, 10**-5, 10**-7]`**:\n",
    "    *   **`n_estimators`**: This parameter defines the number of trees in the forest. A larger number of trees generally improves the model's performance and stability by reducing variance, but at an increased computational cost. The list `[200, 400, 600]` explores a range of forest sizes to balance predictive power and efficiency.\n",
    "    *   **`ccp_alpha` (Minimal Cost-Complexity Pruning)**: This parameter controls the complexity of the individual trees within the forest. Non-negative values of `ccp_alpha` act as a threshold for pruning; trees (or subtrees) that do not offer sufficient improvement in terms of impurity reduction (scaled by `ccp_alpha`) are pruned. Larger `ccp_alpha` values lead to more aggressive pruning and simpler trees, which can help prevent overfitting. The selected range `[10**-2, 10**-3, 10**-5, 10**-7]` allows for testing various degrees of pruning, from relatively strong to very mild.\n",
    "\n",
    "### 2. Time Series Cross-Validation and Data Splitting\n",
    "\n",
    "*   **Decision - `tscv = TimeSeriesSplit(n_splits=n_split)` (e.g., `n_split=3`)**:\n",
    "    *   **Justification**: Standard k-fold cross-validation is inappropriate for time-series data because it can lead to lookahead bias (training on future data to predict the past). `TimeSeriesSplit` ensures that the training set always precedes the validation/test set. In each split, the training set grows, and the test set is the next chronological block of data, mimicking how a model would be deployed in real-time. `n_split=3` creates 3 such chronological train/test splits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c745ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "def time_series_valid_test(X, y, n_split, valid_or_test, n_estimators_list, ccp_alpha_list, optimal_par=None):\n",
    "    np.random.seed(42)\n",
    "    tscv = TimeSeriesSplit(n_splits=n_split)\n",
    "    rf_rmse = []\n",
    "    rmseSmall = []\n",
    "    param_combinations = []  # param combo storagae\n",
    "    i = 0  # fold counter\n",
    "    for train_index, test_index in tscv.split(X):\n",
    "        i += 1\n",
    "        # Break test set into 50% validation set, 50% test set\n",
    "        break_test_ind = int(test_index[0] + 0.5*(test_index[-1]-test_index[0]))\n",
    "        valid_index = np.array(list(range(test_index[0],break_test_ind)))\n",
    "        test_index = np.array(list(range(break_test_ind,test_index[-1])))\n",
    "\n",
    "        # Split data into training, validation, and test sets\n",
    "        X_train, X_valid, X_test = X.iloc[train_index], X.iloc[valid_index], X.iloc[test_index]\n",
    "        y_train, y_valid, y_test = y.iloc[train_index], y.iloc[valid_index], y.iloc[test_index]\n",
    "\n",
    "        # Tuning\n",
    "        if valid_or_test == \"valid\":\n",
    "            rf_rmse_fold = []\n",
    "            for n_estimators in n_estimators_list:\n",
    "                for ccp_alpha in ccp_alpha_list:\n",
    "                    model_rf = RandomForestRegressor(random_state=42,\n",
    "                                                     n_estimators=n_estimators,\n",
    "                                                     ccp_alpha=ccp_alpha,\n",
    "                                                     n_jobs=-1)\n",
    "\n",
    "\n",
    "                    X_train_sample = X_train.sample(frac=0.1, random_state=42)\n",
    "                    y_train_sample = y_train.loc[X_train_sample.index]\n",
    "\n",
    "                    model_rf.fit(X_train_sample, y_train_sample.to_numpy())\n",
    "\n",
    "\n",
    "                    y_val_rf = model_rf.predict(X_valid)\n",
    "                    fold_rmse = np.sqrt(mean_squared_error(y_valid, y_val_rf))\n",
    "                    rf_rmse_fold.append(fold_rmse)\n",
    "                    param_combinations.append((n_estimators, ccp_alpha))\n",
    "\n",
    "            rf_rmse.append(np.mean(rf_rmse_fold))\n",
    "\n",
    "        if valid_or_test == \"test\":\n",
    "            model_rf = RandomForestRegressor(random_state=42,\n",
    "                                             n_estimators=optimal_par[0],\n",
    "                                             ccp_alpha=optimal_par[1],\n",
    "                                             n_jobs=-1)\n",
    "            model_rf.fit(X_train, y_train.to_numpy())\n",
    "            y_test_rf = model_rf.predict(X_test)\n",
    "            test_rmse = np.sqrt(mean_squared_error(y_test, y_test_rf))\n",
    "            rf_rmse.append(test_rmse)\n",
    "\n",
    "            # Plot the prediction for the last CV fold\n",
    "            if i == n_split:\n",
    "                plt.plot(range(len(X_test)), y_test_rf, label=\"Prediction\")\n",
    "                plt.plot(range(len(X_test)), y_test, label=\"True Value\")\n",
    "                plt.legend(loc=\"upper left\")\n",
    "                plt.show()\n",
    "\n",
    "    # Return results\n",
    "    if valid_or_test == \"valid\":\n",
    "        min_rmse_idx = np.argmin(rf_rmse)  # Find index of lowest RMSE\n",
    "        optimal_params = param_combinations[min_rmse_idx]  # Retrieve corresponding parameters\n",
    "        return rf_rmse, optimal_params\n",
    "    if valid_or_test == \"test\":\n",
    "        rf_rmse = np.mean(rf_rmse)\n",
    "        rmseSmallAns = np.mean(rmseSmall)\n",
    "        return y_test_rf, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27a914c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter combinations\n",
    "n_estimators_list = [200, 400, 600]\n",
    "ccp_alpha_list = [10**-2, 10**-3, 10**-5, 10**-7]\n",
    "\n",
    "\n",
    "rf_rmse, optimal_params = time_series_valid_test(X, newy, n_split = 3, valid_or_test = \"valid\",\n",
    "                                                 n_estimators_list = n_estimators_list,\n",
    "                                                 ccp_alpha_list = ccp_alpha_list)\n",
    "print(\"Optimal Parameters:\", optimal_params)\n",
    "print(\"Validation RMSEs (Validation):\", rf_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938a2be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_rf, y_test_actual = time_series_valid_test(X, newy, n_split = 3, valid_or_test = \"test\",\n",
    "                                            n_estimators_list = n_estimators_list,\n",
    "                                            ccp_alpha_list = ccp_alpha_list,\n",
    "                                            optimal_par = optimal_params)\n",
    "\n",
    "print(\"RMSE on test set at final fold:\", np.sqrt(mean_squared_error(y_test_rf, y_test_actual)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf30c9d",
   "metadata": {},
   "source": [
    "Interpretations: Random forest was able to ggenerate a better approximation than LASSO / linear regression. This was performed by taking a time series train-test split and then using k-fold cross validation on a validation set to tune the following parameters to generate the best fitting random forest model: n_estimators and ccp_alpha from the following respective choices: [200,400, 600] and [10-2,10-3, 10-5,10-7]. The goal was to lower the root mean squared error between the two respective data sets, where the RMSE was derived from predictions the model generated at the final fold. For our purposes that was 3. When this was performed on the smaller set, we were able to recieve a rmse of 0.427. Once the model was returned to the larger, more dense dataset, we were able to calibrate a model to achieve a rmse of 0.29. As we would expect, the denser data were able to produce a lower rmse indicating the additional columns in the data matrix were positive additions to the study. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efda51e8",
   "metadata": {},
   "source": [
    "## Extreme Gradient Boost (XGBoost) Regression (Evan Sun)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a14413a",
   "metadata": {},
   "source": [
    "#### Reasons I chose to implement XGBoost:\n",
    "\n",
    "1.  **Superior Predictive Performance and Robustness:**\n",
    "    *   XGBoost is a gradient boosting algorithm renowned for its high predictive accuracy. It often outperforms other algorithms in machine learning competitions and real-world applications (I've lost too many kaggle competitions to XGBoost), including finance.\n",
    "    *   The ensemble nature of XGBoost, where multiple decision trees are built sequentially with each new tree correcting the errors of the previous ones, leads to a robust model that is less prone to individual tree weaknesses.\n",
    "\n",
    "2.  **Effective Handling of Non-Linearity and Feature Interactions:**\n",
    "    *   Financial markets, and specifically price responses to news events like the EIA WPSR, rarely exhibit simple linear relationships. Price changes can be influenced by a complex interplay of various factors (e.g., the magnitude of the surprise in the report, prevailing market sentiment, existing inventory levels, recent price trends).\n",
    "    *   XGBoost, being a tree-based model, excels at capturing these non-linear relationships and high-order interactions between features without requiring explicit feature engineering for these interactions (e.g., creating polynomial terms).\n",
    "\n",
    "3.  **Built-in Regularization to Prevent Overfitting:**\n",
    "    *   Financial data is notoriously noisy, and overfitting is a significant risk. XGBoost incorporates both L1 (Lasso) and L2 (Ridge) regularization terms in its objective function.\n",
    "    *   This regularization helps to penalize model complexity, shrink less important feature weights, and thus improve the model's ability to generalize to unseen data – crucial for predicting future price movements after a WPSR release. Your hyperparameter grid explicitly tunes `reg_alpha` (L1) and `reg_lambda` (L2), leveraging this strength.\n",
    "\n",
    "4.  **Computational Efficiency and Scalability:**\n",
    "    *   XGBoost is designed for speed and efficiency. It implements parallel processing for tree construction and cache-aware access patterns.\n",
    "\n",
    "<!--also the name is cool lol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e4a3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create training and test split\n",
    "column_names = []\n",
    "variable_names = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "for var_name in variable_names:\n",
    "    for min in range(60):\n",
    "        column_names.append(f'{var_name}_t{min-60}')\n",
    "\n",
    "column_names.append('Open_t0')\n",
    "\n",
    "new_cols = ['Actual', 'Forecast', 'Previous', 'Weekly Net Import','Weekly Production']\n",
    "for col_name in new_cols:\n",
    "    column_names.append(col_name)\n",
    "\n",
    "n_total_events = len(full_data)\n",
    "test_set_size_ratio = 0.2\n",
    "\n",
    "test_set_split_index = int(n_total_events * (1 - test_set_size_ratio))\n",
    "\n",
    "# Data for hyperparameter tuning and training the final model\n",
    "X_tuning_full = full_data.iloc[:test_set_split_index][column_names]\n",
    "y_tuning_full = full_data.iloc[:test_set_split_index]['Pct_Change']\n",
    "\n",
    "# Final hold-out test set (never seen during tuning)\n",
    "X_test_holdout = full_data.iloc[test_set_split_index:][column_names]\n",
    "y_test_holdout = full_data.iloc[test_set_split_index:]['Pct_Change']\n",
    "\n",
    "print(f\"Full dataset size: {len(df_wide)}\")\n",
    "print(f\"Tuning dataset size: {len(X_tuning_full)}\")\n",
    "print(f\"Hold-out test dataset size: {len(X_test_holdout)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bbcc76",
   "metadata": {},
   "source": [
    "### Design Justifications:\n",
    "\n",
    "Some of the design decisions made might not be conventional and thus can be explained as follows:\n",
    "\n",
    "### 1. Hyperparameters Grid\n",
    "\n",
    "*   **Decision - `'n_estimators': [1000]`**: I chose the max value here since I'm using early stopping so I can let early stopping determine the optimal number of trees dynamically for each hyperparameter combination and fold. The `actual_n_estimators` is then recorded and averaged.\n",
    "\n",
    "### 2. Time Series Cross-Validation\n",
    "\n",
    "*   **Decision - `'tscv = TimeSeriesSplit(n_splits=3)`**: I'm using a time series split because for financial time series data (like price changes), it's crucial that the validation set always comes *after* the training set to prevent lookahead bias and realistically simulate how the model would be used in practice (training on past data to predict future data).\n",
    "\n",
    "### 3. Early Stopping\n",
    "\n",
    "*   **Decision - `'early_stopping_rounds_val = 10`**: I chose to use early stopping for 2 reasons:\n",
    "    *  Prevents the model from fitting the training data too closely and losing generalization ability.\n",
    "    *  saves computation time by not necessarily training for the full `n_estimators` if optimal performance is achieved earlier.\n",
    "    \n",
    "### 4. Sample Weighting\n",
    "\n",
    "*   **Decision - `'epsilon = 1e-6`**:\n",
    "    *   **Decision**: Add a small constant to target values before taking absolute value for weights.\n",
    "    *   **Justification**: This is to prevent issues if any `y_tuning_full` or `y_test_holdout` values are exactly zero. `np.abs(0)` is 0, which would result in a zero weight, effectively ignoring that sample. `epsilon` ensures all samples have at least a tiny positive weight.\n",
    "\n",
    "*   **Decision - `weights_tuning_full = np.abs(y_tuning_full) + epsilon`**: I noticed that the 2-minute price changes were often very close to 0 and as a result I decided to implement sample weighting so that the model is incentivized to predict more important/larger price changes more accurately. This way the model doesn't default to just predicting 0 for most events and basically being useless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02af696e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train and tune model\n",
    "hyperparam_grid = {\n",
    "    'n_estimators': [1000],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [6, 7, 8],\n",
    "    'subsample': [0.85, 0.9, 0.95],\n",
    "    'colsample_bytree': [0.85, 0.9, 0.95],\n",
    "    'reg_alpha': [0.1, 0.3, 0.5],\n",
    "    'reg_lambda': [3, 4, 5],\n",
    "}\n",
    "\n",
    "param_list = list(ParameterGrid(hyperparam_grid))\n",
    "\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "results = []\n",
    "early_stopping_rounds_val = 10\n",
    "\n",
    "\n",
    "epsilon = 1e-6\n",
    "weights_tuning_full = np.abs(y_tuning_full) + epsilon\n",
    "weights_test_holdout = np.abs(y_test_holdout) + epsilon\n",
    "\n",
    "\n",
    "print(f\"Starting hyperparameter tuning with TimeSeriesSplit. Total combinations: {len(param_list)}\")\n",
    "for fold, (train_index, val_index) in enumerate(tscv.split(X_tuning_full)):\n",
    "    print(f\"\\nFold {fold+1}/{tscv.get_n_splits()}\")\n",
    "    X_train_fold, y_train_fold = X_tuning_full.iloc[train_index], y_tuning_full.iloc[train_index]\n",
    "    X_val_fold, y_val_fold = X_tuning_full.iloc[val_index], y_tuning_full.iloc[val_index]\n",
    "    weights_train_fold = weights_tuning_full.iloc[train_index]\n",
    "    weights_val_fold = weights_tuning_full.iloc[val_index]\n",
    "\n",
    "    print(f\"  Training on {len(X_train_fold)} instances, validating on {len(X_val_fold)} instances.\")\n",
    "\n",
    "\n",
    "    for i, params in enumerate(param_list):\n",
    "        if (i + 1) % 10 == 0 or i == 0 or i == len(param_list) -1 :\n",
    "             print(f\"    Combination {i+1}/{len(param_list)}: {params}\")\n",
    "\n",
    "        model = xgb.XGBRegressor(\n",
    "            objective='reg:squarederror',\n",
    "            random_state=42,\n",
    "            early_stopping_rounds=early_stopping_rounds_val,\n",
    "            n_jobs=-1,\n",
    "            **params\n",
    "        )\n",
    "\n",
    "        model.fit(X_train_fold, y_train_fold,\n",
    "                  sample_weight=weights_train_fold,\n",
    "                  eval_set=[(X_val_fold, y_val_fold)],\n",
    "                  sample_weight_eval_set=[weights_val_fold],\n",
    "                  verbose=False)\n",
    "\n",
    "        y_pred_val = model.predict(X_val_fold)\n",
    "        val_rmse = np.sqrt(mean_squared_error(y_val_fold, y_pred_val, sample_weight=weights_val_fold))\n",
    "\n",
    "        result_entry = params.copy()\n",
    "        result_entry['fold'] = fold + 1\n",
    "        result_entry['val_rmse'] = val_rmse\n",
    "        result_entry['actual_n_estimators'] = model.best_iteration\n",
    "        results.append(result_entry)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "grouping_cols = [col for col in hyperparam_grid.keys() if col != 'n_estimators']\n",
    "average_metrics_across_folds = results_df.groupby(grouping_cols).agg(\n",
    "    avg_val_rmse=('val_rmse', 'mean'),\n",
    "    avg_actual_n_estimators=('actual_n_estimators', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "\n",
    "best_params_row = average_metrics_across_folds.loc[average_metrics_across_folds['avg_val_rmse'].idxmin()]\n",
    "\n",
    "best_hyperparams = best_params_row.to_dict()\n",
    "best_hyperparams['max_depth'] = int(best_hyperparams.get('max_depth'))\n",
    "best_avg_rmse = best_hyperparams.pop('avg_val_rmse')\n",
    "best_actual_n_estimators = int(round(best_hyperparams.pop('avg_actual_n_estimators')))\n",
    "\n",
    "\n",
    "print(\"\\nBest hyperparameters found via TimeSeriesSplit CV:\")\n",
    "for param_name, param_val in best_hyperparams.items():\n",
    "    print(f\"  {param_name}: {param_val}\")\n",
    "print(f\"  (Best actual n_estimators found by early stopping, averaged): {best_actual_n_estimators}\")\n",
    "print(f\"  Average Validation RMSE across folds: {best_avg_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa611dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate model on test set\n",
    "print(\"\\n--- Training Final Model with Best Hyperparameters ---\")\n",
    "params_for_final_model = best_hyperparams.copy()\n",
    "\n",
    "final_model = xgb.XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    n_estimators=best_actual_n_estimators,\n",
    "    **params_for_final_model\n",
    ")\n",
    "\n",
    "print(f\"Training final model on {len(X_tuning_full)} instances.\")\n",
    "final_model.fit(X_tuning_full, y_tuning_full,\n",
    "                sample_weight=weights_tuning_full,\n",
    "                verbose=False)\n",
    "\n",
    "print(\"\\n--- Evaluating Final Model on Hold-Out Test Set ---\")\n",
    "y_pred_test_holdout = final_model.predict(X_test_holdout)\n",
    "\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test_holdout, y_pred_test_holdout,\n",
    "                                       sample_weight=weights_test_holdout))\n",
    "\n",
    "print(f\"Final Model Performance on Hold-Out Test Set:\")\n",
    "print(f\"  Test RMSE: {test_rmse:.6f}\")\n",
    "\n",
    "baseline_pred_zero = np.zeros_like(y_test_holdout)\n",
    "baseline_rmse_zero = np.sqrt(mean_squared_error(y_test_holdout, baseline_pred_zero,\n",
    "                                                 sample_weight=weights_test_holdout))\n",
    "\n",
    "print(f\"\\nBaseline Model (Predict Zero Change) Performance on Hold-Out Test Set:\")\n",
    "print(f\"  Baseline RMSE: {baseline_rmse_zero:.6f}\")\n",
    "\n",
    "try:\n",
    "    feature_importances = final_model.feature_importances_\n",
    "    importance_df = pd.DataFrame({'feature': X_tuning_full.columns, 'importance': feature_importances})\n",
    "    importance_df = importance_df.sort_values(by='importance', ascending=False)\n",
    "    print(\"\\nTop 10 Feature Importances from Final Model:\")\n",
    "    print(importance_df.head(10))\n",
    "except Exception as e:\n",
    "    print(f\"Could not retrieve feature importances: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5647e05f",
   "metadata": {},
   "source": [
    "**Evaluation of Results:**\n",
    "\n",
    "1.  **Model Performance vs. Baseline:**\n",
    "    *   **Final Model Test RMSE: 0.393174**\n",
    "    *   **Baseline Model (Predict Zero Change) RMSE: 0.402660**\n",
    "\n",
    "    *   **Interpretation:** My XGBoost model has a lower RMSE on the hold-out test set than the naive baseline model (which simply predicts no price change). This indicates that the model has learned some predictive patterns from the features and is providing more accurate predictions than simply assuming the price will remain static over the next 2 minutes.\n",
    "    *   **Magnitude of Improvement:** The improvement in my model's RMSE is approximately `(0.009486 / 0.402660) * 100% ≈ 2.35%` lower than the baseline.\n",
    "\n",
    "2.  **Top 10 Feature Importances:**\n",
    "\n",
    "    *   `261 Volume_t-39 (0.206890)`: This is by far the **most dominant feature**, contributing over 20% to the model's predictive power (based on this importance type, likely 'gain' or 'weight'). It suggests that the trading volume 39 minutes *before the WPSR release* is a very strong indicator of the subsequent price change. This could reflect anticipatory trading or positioning by informed market participants.\n",
    "    *   `248 Volume_t-52 (0.066634)`\n",
    "    *   `0 Open_t-60 (0.059081)`\n",
    "    *   `295 Volume_t-5 (0.055731)`\n",
    "    *   `251 Volume_t-49 (0.053814)`\n",
    "    *   `271 Volume_t-29 (0.051934)`\n",
    "    *   `297 Volume_t-3 (0.0359"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4df97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 6))\n",
    "time_indices = X_test_holdout.index\n",
    "\n",
    "true_changes_flat = y_test_holdout.flatten() if hasattr(y_test_holdout, 'flatten') else y_test_holdout\n",
    "predicted_changes_flat = y_pred_test_holdout.flatten() if hasattr(y_pred_test_holdout, 'flatten') else y_pred_test_holdout\n",
    "comparison_df = pd.DataFrame({\n",
    "    'True Change': true_changes_flat,\n",
    "    'Predicted Change': predicted_changes_flat\n",
    "})\n",
    "plt.plot(time_indices, comparison_df['True Change'], label='True Change', marker='.', linestyle='-', alpha=0.8)\n",
    "plt.plot(time_indices, comparison_df['Predicted Change'], label='Predicted Change', marker='.', linestyle='-', alpha=0.8)\n",
    "plt.xlabel(\"Sample Index / Time\")\n",
    "plt.ylabel(\"Change\")\n",
    "plt.title(f\"True vs. Predicted Change Over Time\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout() # Adjust layout to make room for labels\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a32dd34",
   "metadata": {},
   "source": [
    "## Deep Neural Network (Terence Chiu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89eed39d",
   "metadata": {},
   "source": [
    "Overview  \n",
    "\n",
    "This model implements a Deep Neural Network (DNN) to predict percentage changes in price following economic data releases. The model takes into account both historical price data and weekly economic indicators to make predictions.\n",
    "\n",
    "Model Architecture  \n",
    "\n",
    "- **Input Layer**: 7 features\n",
    "  - 4 price-related features: Close prices at t-60, t-40, t-20, and Open price at t0\n",
    "  - 3 weekly economic indicators: Weekly Production, Weekly Net Import, and Actual Supply\n",
    "- **Hidden Layers**: \n",
    "  - First layer: 32 neurons with ReLU activation\n",
    "  - Second layer: 8 neurons with ReLU activation\n",
    "  - Dropout rate of 0.2 after each hidden layer\n",
    "- **Output Layer**: Single neuron for percentage change prediction\n",
    "- **Learning Rate**: 0.000187 (optimized through hyperparameter tuning, check terenceTuningModel.py)\n",
    "\n",
    "Data Processing  \n",
    "- **Feature Scaling**: All input features are standardized using StandardScaler\n",
    "- **Target Variable**: Percentage change in price 2 minutes after release\n",
    "- **Train-Test Split**: 80-20 time-based split to maintain temporal order\n",
    "- **Sample Weighting**: Implemented to give more importance to larger price movements\n",
    "\n",
    "Training Process  \n",
    "- **Optimizer**: Adam optimizer - Adapts learning rates for each parameter individually\n",
    "\n",
    "- **Loss Function**: Mean Squared Error (MSE) -   - Aligns with our goal of minimizing prediction errors since it penalizes larger errors\n",
    "- **Metrics**: \n",
    "  - Custom RMSE metric - matches evaluation metric used in financial analysis, and more sensisitive to outliers than MAE\n",
    "  - Mean Absolute Error (MAE) - robost for extreme data\n",
    "- **Early Stopping**: - Prevent overfitting\n",
    "  - Monitors validation RMSE\n",
    "  - Patience of 10 epochs\n",
    "  - Restores best weights\n",
    "\n",
    "Model Performance  \n",
    "- **Evaluation Metric**: Weighted RMSE\n",
    "- **Baseline Comparison**: Model performance is compared against a baseline of predicting zero change\n",
    "- **Feature Importance**: Analyzed using gradient-based importance scores to understand feature contributions\n",
    "\n",
    "Key Innovations  \n",
    "1. **Sample Weighting**: Implementation of sample weights to handle the imbalance between small and large price movements\n",
    "2. **Custom RMSE Metric**: Development of a custom RMSE metric for better model evaluation\n",
    "3. **Feature Importance Analysis**: Gradient-based analysis to understand feature contributions\n",
    "4. **Time-Based Split**: Maintaining temporal order in train-test split to prevent look-ahead bias\n",
    "\n",
    "Model Limitations  \n",
    "1. Relies on only four prices within 60 minutes prior to weekly report release\n",
    "2. Does not look at whole market fluctuations\n",
    "3. Performance depends on the quality and timeliness of the economic data releases\n",
    "\n",
    "Future Improvements  \n",
    "1. Experiment with different network architectures, such as LSTM, CNN?\n",
    "2. Incorporate additional features or market indicators\n",
    "3. Implement ensemble methods with other models\n",
    "4. Explore different loss functions for better handling of extreme price movements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f939bd1",
   "metadata": {},
   "source": [
    "Data Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0095f53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "df = pd.read_csv(\"full_data.csv\")\n",
    "\n",
    "feature_cols = [col for col in df.columns if 'Close_t-60'  in col or 'Close_t-40' in col or 'Close_t-20' in col or col == 'Release Date' or col == 'Actual' or col == 'Weekly Net Import' or col == 'Weekly Production' or col == 'Open_t0']\n",
    "\n",
    "X_temp = df[feature_cols]\n",
    "y_temp = (df['Close_t2'] - df['Close_t0'])/df['Close_t0']\n",
    "\n",
    "prod_weekly = X_temp[['Release Date', 'Weekly Production']]\n",
    "net_import_weekly = X_temp[['Release Date', 'Weekly Net Import']]\n",
    "supply_weekly = X_temp[['Release Date', 'Actual']]\n",
    "price_wide = X_temp[['Release Date', 'Close_t-60', 'Close_t-40', 'Close_t-20', 'Open_t0']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06c8714",
   "metadata": {},
   "source": [
    "Data Scaling - for price features, weekly features, and target price feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a088148",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_scaler = StandardScaler()\n",
    "target_scaler = StandardScaler()\n",
    "\n",
    "price_features = price_wide[['Close_t-60', 'Close_t-40', 'Close_t-20', 'Open_t0']]\n",
    "\n",
    "# Scale the price features in the dataframe\n",
    "for col in ['Close_t-60', 'Close_t-40', 'Close_t-20', 'Open_t0']:\n",
    "    price_features[col] = price_scaler.fit_transform(price_wide[col].values.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Scale the target values in the dataframe\n",
    "y_temp = target_scaler.fit_transform(y_temp.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "#Scaler for weekly data\n",
    "weekly_scaler = StandardScaler()\n",
    "\n",
    "weekly_production_scaled = weekly_scaler.fit_transform(prod_weekly['Weekly Production'].values.reshape(-1, 1)).flatten()\n",
    "weekly_import_scaled = weekly_scaler.fit_transform(net_import_weekly['Weekly Net Import'].values.reshape(-1, 1)).flatten()\n",
    "weekly_supply_scaled = weekly_scaler.fit_transform(supply_weekly['Actual'].values.reshape(-1, 1)).flatten()\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for idx, row in price_features.iterrows():\n",
    "    # Target: price of future 2 minutes after release (already scaled)\n",
    "    target_price = y_temp[idx]\n",
    "\n",
    "\n",
    "    production_value = weekly_production_scaled[idx]\n",
    "    import_value = weekly_import_scaled[idx]\n",
    "    supply_value = weekly_supply_scaled[idx]\n",
    "\n",
    "    row_data = [price_features['Close_t-60'].values[idx],price_features['Close_t-40'].values[idx],price_features['Close_t-20'].values[idx],price_features['Open_t0'].values[idx],production_value,import_value,supply_value]\n",
    "    X.append(row_data)\n",
    "    y.append(target_price)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b03b2b",
   "metadata": {},
   "source": [
    "Importing Model from terenceModel.py.\n",
    "Model decision/architecture decided from terenceTuneModel using keras_tuner.\n",
    "Best hyperparameters found from tuning    \n",
    "   \n",
    "    1. Best configuration:  \n",
    "    2. 2 layers: [32 units, 0.2 dropout], [8 units, 0.2 dropout]  \n",
    "    3. Learning rate: 0.000187\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c16580",
   "metadata": {},
   "outputs": [],
   "source": [
    "from terenceModel import DNN\n",
    "from terenceTrainModel import plot_predictions\n",
    "\n",
    "# Time-based 80/20 split\n",
    "n = len(X)\n",
    "split_idx = int(n * 0.8)\n",
    "X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "y_train, y_test = y[:split_idx], y[split_idx:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55c51a0",
   "metadata": {},
   "source": [
    "We applied sample weights in training of model, and the calculation of RMSE, as (close to) 0 percent change was something that was likely to occur.\n",
    "This model will be shown to be outperforming the baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbcd2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute sample weights for training and test sets\n",
    "epsilon = 1e-6\n",
    "sample_weights_train = np.abs(y_train) + epsilon\n",
    "sample_weights_test = np.abs(y_test) + epsilon\n",
    "\n",
    "model = DNN()\n",
    "\n",
    "# Train model with sample weights\n",
    "trained_model, _ = model.train(X_train, y_train, sample_weight=sample_weights_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = trained_model.predict(X_test).flatten()\n",
    "\n",
    "# Weighted RMSE for test set\n",
    "weighted_test_rmse = np.sqrt(mean_squared_error(y_test, y_pred, sample_weight=sample_weights_test))\n",
    "print(f\"Weighted Test RMSE: {weighted_test_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81b6c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_pred_zero = np.zeros_like(y_test)\n",
    "baseline_rmse_zero = np.sqrt(mean_squared_error(y_test, baseline_pred_zero, sample_weight=sample_weights_test))\n",
    "print(f\"Baseline (Zero Prediction) Weighted RMSE: {baseline_rmse_zero:.4f}\")\n",
    "\n",
    "if weighted_test_rmse < baseline_rmse_zero:\n",
    "    print(\"Model outperforms baseline (predicting zero change).\")\n",
    "else:\n",
    "    print(\"Model does NOT outperform baseline (predicting zero change).\")\n",
    "\n",
    "plot_predictions(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a0d838",
   "metadata": {},
   "source": [
    "We compare with the weightless model with the same settings except that this model does not train with sample_weights, and RMSE calculation does not use sample_weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700a873c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run terenceTrainWlessModel.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bcf4e3",
   "metadata": {},
   "source": [
    "We look at feature importance of the model with sample weights (the one that outperforms baseline model). Check terenceTrainModel functions that uses gradient to look at feature importance using TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802c4e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from terenceTrainModel import compute_feature_importance, plot_feature_importance\n",
    "\n",
    "feature_names = ['Close_t-60', 'Close_t-40', 'Close_t-20', 'Open_t0',\n",
    "                    'Weekly Production', 'Weekly Net Import', 'Actual']\n",
    "feature_importance = compute_feature_importance(trained_model, X_test)\n",
    "\n",
    "# Print feature importance\n",
    "print(\"\\nFeature Importance (based on gradients):\")\n",
    "for name, importance in zip(feature_names, feature_importance):\n",
    "    print(f\"{name}: {importance:.6f}\")\n",
    "\n",
    "# Plot feature importance\n",
    "plot_feature_importance(feature_importance, feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3bfd5d",
   "metadata": {},
   "source": [
    "Of the future prices: t-20 and t-60 have the biggest importance in predicting the price two minutes after report release.  \n",
    "\n",
    "Of the weekly data: Weekly Production of Crude Oil have the biggest importance, and then net import.  \n",
    "\n",
    "Across the 7 features, all are important, none are drastically insignificant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed99a1a",
   "metadata": {},
   "source": [
    "## KNN (Samyak Kapoor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f21322e",
   "metadata": {},
   "source": [
    "For this project, I selected the K-Nearest Neighbors (KNN) algorithm as one of the predictive models. KNN is a non-parametric, instance-based learning method that is particularly well-suited to my core hypothesis: that the market's reaction to a new EIA report will be similar to its reaction during past events that shared similar characteristics.\n",
    "\n",
    "Instead of trying to find a global mathematical function mapping features to an outcome, KNN operates on a simple and intuitive principle of \"similarity.\" It classifies a new event by identifying the k most similar events (the \"nearest neighbors\") from the historical training data and taking a majority vote of their outcomes. This approach is powerful because it makes no assumptions about the linearity or distribution of the financial data, which is notoriously complex and non-stationary. My goal is to see if this direct, similarity-based approach can identify recurring patterns in the pre-report market dynamics and fundamental surprises to predict the subsequent price movement.\n",
    "\n",
    "Prediction Target: The objective of this model is to predict the short-term directional movement of WTI crude oil futures in the immediate aftermath of the EIA report. I have framed this as a binary classification task.\n",
    "\n",
    "Target Calculation: The target variable is derived from the percentage price change between the opening price of the release minute (Open_t0) and the closing price two minutes later (Close_t2).\n",
    "\n",
    "Percentage Change = (Close_t2 - Open_t0) / Open_t0\n",
    "\n",
    "Class Binarization: This continuous percentage change is then converted into two discrete classes, representing a simple \"up\" or \"down\" trading decision:\n",
    "Class 1 (UP): Assigned if the Percentage Change is positive (> 0).\n",
    "Class 0 (DOWN): Assigned if the Percentage Change is zero or negative (<= 0).\n",
    "\n",
    "Crucially, in adherence with a realistic trading scenario, the model is built under the constraint that it cannot use any market information after the release (t0) except for Open_t0, which is required to establish the entry point for my hypothetical trade. All predictive features are generated from data available at or before t-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9cb322",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "master_df = pd.read_csv(\"full_data.csv\", index_col=1, parse_dates=True)\n",
    "master_df.columns = master_df.columns.str.strip()\n",
    "\n",
    "# A) Add Fundamental Features\n",
    "master_df['funda_surprise'] = master_df['Actual'] - master_df['Forecast']\n",
    "master_df['funda_change_vs_prev'] = master_df['Actual'] - master_df['Previous']\n",
    "master_df['funda_import_pct_chg'] = master_df['Weekly Net Import'].pct_change()\n",
    "master_df['funda_prod_pct_chg'] = master_df['Weekly Production'].pct_change()\n",
    "\n",
    "# B) Add Market Features\n",
    "market_feature_names = [\n",
    "    'mkt_ret_5m', 'mkt_ret_15m', 'mkt_ret_30m', 'mkt_ret_60m',\n",
    "    'mkt_vol_15m', 'mkt_vol_60m', 'mkt_vol_trend_10m_60m',\n",
    "    'mkt_bar_body_size', 'mkt_bar_upper_wick'\n",
    "]\n",
    "\n",
    "for index, event_row in master_df.iterrows():\n",
    "    ts_df = pd.DataFrame(columns=['Open', 'High', 'Low', 'Close', 'Volume'])\n",
    "    for t in range(-60, 0):\n",
    "        ts_df.loc[t] = [event_row.get(f'Open_t{t}'), event_row.get(f'High_t{t}'),\n",
    "                        event_row.get(f'Low_t{t}'), event_row.get(f'Close_t{t}'),\n",
    "                        event_row.get(f'Volume_t{t}')]\n",
    "\n",
    "    if ts_df.isnull().values.any(): continue\n",
    "    price_anchor = ts_df.loc[-1, 'Close']; volume_anchor = ts_df['Volume'].mean()\n",
    "    if pd.isna(price_anchor) or price_anchor == 0 or pd.isna(volume_anchor) or volume_anchor == 0: continue\n",
    "\n",
    "    norm_ts_df = ts_df.copy()\n",
    "    norm_ts_df[['Open','High','Low','Close']] = (ts_df[['Open','High','Low','Close']] / price_anchor) - 1\n",
    "    norm_ts_df['Volume'] = ts_df['Volume'] / volume_anchor\n",
    "    norm_ts_df['min_ret'] = norm_ts_df['Close'].diff()\n",
    "\n",
    "    master_df.loc[index, 'mkt_ret_5m'] = norm_ts_df.loc[-1,'Close'] - norm_ts_df.loc[-5,'Close']\n",
    "    master_df.loc[index, 'mkt_ret_15m'] = norm_ts_df.loc[-1,'Close'] - norm_ts_df.loc[-15,'Close']\n",
    "    master_df.loc[index, 'mkt_ret_30m'] = norm_ts_df.loc[-1,'Close'] - norm_ts_df.loc[-30,'Close']\n",
    "    master_df.loc[index, 'mkt_ret_60m'] = norm_ts_df.loc[-1,'Close'] - norm_ts_df.loc[-60,'Close']\n",
    "    master_df.loc[index, 'mkt_vol_15m'] = norm_ts_df['min_ret'].iloc[-15:].std()\n",
    "    master_df.loc[index, 'mkt_vol_60m'] = norm_ts_df['min_ret'].iloc[-60:].std()\n",
    "    master_df.loc[index, 'mkt_vol_trend_10m_60m'] = norm_ts_df['Volume'].iloc[-10:].mean() / norm_ts_df['Volume'].mean()\n",
    "    last_bar=norm_ts_df.loc[-1]\n",
    "    master_df.loc[index, 'mkt_bar_body_size'] = abs(last_bar['Close']-last_bar['Open'])\n",
    "    master_df.loc[index, 'mkt_bar_upper_wick'] = last_bar['High']-max(last_bar['Open'],last_bar['Close'])\n",
    "\n",
    "all_feature_names = [\n",
    "    'funda_surprise', 'funda_change_vs_prev', 'funda_import_pct_chg', 'funda_prod_pct_chg'\n",
    "] + market_feature_names\n",
    "\n",
    "final_df = master_df.dropna(subset=all_feature_names + ['Close_t2', 'Open_t0'])\n",
    "\n",
    "X = final_df[all_feature_names]\n",
    "y_raw = (final_df['Close_t2'] - final_df['Open_t0']) / final_df['Open_t0']\n",
    "y = (y_raw > 0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da114751",
   "metadata": {},
   "source": [
    "This code performs the complete data preparation and feature engineering pipeline necessary to transform the raw dataset into a format suitable for my KNN model. I began by loading the consolidated data file and setting the report's Release Date as a time-series index. I then systematically enriched the dataset by creating thirteen high-level, predictive features.\n",
    "\n",
    "Four of these features are fundamental, designed to quantify the economic surprise of the EIA report. This includes the critical funda_surprise feature (the difference between actual and forecasted supply) as well as normalized week-over-week changes in production and net imports.\n",
    "\n",
    "The remaining nine features capture the market dynamics in the hour preceding the report. To generate these, I reshaped each event's flat price history into a temporary time-series and normalized it to remove biases from absolute price levels, and then calculates metrics summarizing market momentum, volatility, and volume trends. After all features were generated, I cleaned the entire dataset by removing any event with incomplete data, resulting in the final, aligned feature matrix X and target vector y used for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c233d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "\n",
    "# 1. Chronological Data Split\n",
    "split_point = int(len(X) * 0.80)\n",
    "X_train, X_test = X.iloc[:split_point], X.iloc[split_point:]\n",
    "y_train, y_test = y.iloc[:split_point], y.iloc[split_point:]\n",
    "\n",
    "\n",
    "# 2. Data Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 3. Hyperparameter Tuning\n",
    "k_values = range(1,10)\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "cv_scores = []\n",
    "\n",
    "for k in k_values:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn, X_train_scaled, y_train, cv=tscv, scoring='roc_auc')\n",
    "    cv_scores.append(scores.mean())\n",
    "best_k = k_values[np.argmax(cv_scores)]\n",
    "print(f\"Optimal 'k' found: {best_k} with average CV ROC AUC of {max(cv_scores):.4f}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "# --- 4. Final Model Training and Evaluation ---\n",
    "final_knn = KNeighborsClassifier(n_neighbors=best_k)\n",
    "final_knn.fit(X_train_scaled, y_train)\n",
    "y_pred = final_knn.predict(X_test_scaled)\n",
    "y_pred_proba = final_knn.predict_proba(X_test_scaled)[:, 1]\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "print(f\"Model: KNN with k={best_k}\")\n",
    "print(f\"Out-of-Sample Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Out-of-Sample ROC AUC Score: {roc_auc:.4f}\")\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(pd.DataFrame(confusion_matrix(y_test, y_pred),\n",
    "                 index=['Actual DOWN', 'Actual UP'],\n",
    "                 columns=['Predicted DOWN', 'Predicted UP']))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['DOWN', 'UP']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b903a71",
   "metadata": {},
   "source": [
    "To assess the predictive capability of the K-Nearest Neighbors (KNN) algorithm, I implemented a rigorous training and evaluation pipeline designed to adhere to time-series best practices and prevent look-ahead bias.\n",
    "\n",
    "I began by partitioning the dataset chronologically, using the initial 80% of events for model training and reserving the final 20% as a completely unseen hold-out test set. To ensure that features with larger numerical scales did not disproportionately influence the distance-based KNN algorithm, all predictive features were standardized. This scaling was performed by fitting a StandardScaler.\n",
    "\n",
    "A critical part of the process was to determine the optimal hyperparameter k (the number of neighbors). This was achieved through a nested cross-validation procedure on the training set using TimeSeriesSplit, which preserves the temporal order of financial data. This tuning process systematically searched for the value of k that maximized the average ROC AUC score, thereby identifying the model configuration best suited to finding patterns in the historical data without consulting the final test set.\n",
    "\n",
    "Finally, a KNN model was instantiated with this optimal k and trained on the entire training dataset. The model's true predictive power was then evaluated by making predictions on the hold-out test set.\n",
    "\n",
    "Results and Interpretation\n",
    "The hyperparameter tuning phase identified an optimal value of k=5, suggesting that a small set of neighboring historical events provides the most stable signal. \n",
    "\n",
    "A ROC AUC score meaningfully above the 0.5 baseline of random chance indicates that the model possesses a modest but statistically valid predictive edge. It is able to distinguish between positive and negative post-announcement price movements better than a coin flip. However, a deeper analysis of the model's predictions reveals a significant performance asymmetry. While the model showed some ability to correctly identify \"UP\" movements, its precision for \"DOWN\" movements was low, indicating it was less reliable when predicting price decreases. This suggests that while a quantifiable signal exists within the engineered features, the KNN model's ability to capitalize on it is limited, particularly for identifying upward price moves.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5691eaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "knn_model = KNeighborsClassifier()\n",
    "param_grid = {'n_neighbors': range(1,10)}\n",
    "n_splits = 4\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits, test_size=147)\n",
    "\n",
    "fold_results = []\n",
    "all_y_test_agg = []\n",
    "all_y_pred_agg = []\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(tscv.split(X)):\n",
    "    fold_num = fold + 1\n",
    "    print(f\"\\n Fold {fold_num}/{n_splits}\")\n",
    "\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    inner_tscv = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "    print(f\"Tuning 'k' for Fold {fold_num}...\")\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=knn_model,\n",
    "        param_grid=param_grid,\n",
    "        cv=inner_tscv,\n",
    "        scoring='roc_auc',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "    best_k = grid_search.best_params_['n_neighbors']\n",
    "    best_knn_for_fold = grid_search.best_estimator_\n",
    "    y_pred = best_knn_for_fold.predict(X_test_scaled)\n",
    "    y_pred_proba = best_knn_for_fold.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "    fold_results.append({\n",
    "        'Fold': fold_num,\n",
    "        'ROC AUC': roc_auc_score(y_test, y_pred_proba),\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'Best k': best_k,\n",
    "    })\n",
    "    all_y_test_agg.extend(y_test)\n",
    "    all_y_pred_agg.extend(y_pred)\n",
    "\n",
    "    print(f\"  > Fold {fold_num} Test ROC AUC: {fold_results[-1]['ROC AUC']:.4f} (Found optimal k={best_k})\")\n",
    "\n",
    "\n",
    "# A) Overall Performance Metrics for all folds\n",
    "print(\"Overall Out-of-Sample Performance (Aggregated)\")\n",
    "print(f\"Total Predictions Made: {len(all_y_test_agg)}\")\n",
    "print(f\"Overall Accuracy: {accuracy_score(all_y_test_agg, all_y_pred_agg):.4f}\\n\")\n",
    "print(\"Overall Classification Report:\")\n",
    "print(classification_report(all_y_test_agg, all_y_pred_agg, target_names=['DOWN', 'UP']))\n",
    "\n",
    "# B) Fold-by-Fold Performance Breakdown\n",
    "results_df = pd.DataFrame(fold_results).set_index('Fold')\n",
    "print(\"\\n--- Detailed Performance per Fold ---\")\n",
    "print(results_df.round(4))\n",
    "\n",
    "# C) Final Summary Statistics\n",
    "print(\"\\n--- Final Performance Summary ---\")\n",
    "print(f\"Mean Out-of-Sample ROC AUC: {results_df['ROC AUC'].mean():.4f}\")\n",
    "print(f\"Std Dev of ROC AUC:       {results_df['ROC AUC'].std():.4f}\")\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8), sharex=True)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "# Plot 1: ROC AUC Score per Fold\n",
    "results_df['ROC AUC'].plot(kind='bar', ax=ax1, color='teal', edgecolor='black')\n",
    "ax1.axhline(0.5, ls='--', color='red', label='Random Guess (0.5)')\n",
    "ax1.set_title('KNN Performance per Walk-Forward Fold', fontsize=14)\n",
    "ax1.set_ylabel('Out-of-Sample ROC AUC', fontsize=12)\n",
    "ax1.legend()\n",
    "ax1.tick_params(axis='x', rotation=0)\n",
    "ax1.set_ylim(bottom=min(0.4, results_df['ROC AUC'].min() - 0.05))\n",
    "\n",
    "# Plot 2: Best 'k' Found per Fold\n",
    "results_df['Best k'].plot(kind='line', ax=ax2, marker='o', color='darkorange', label='Optimal k')\n",
    "ax2.set_title('Optimal \"k\" Found in Each Fold', fontsize=14)\n",
    "ax2.set_xlabel('Fold Number', fontsize=12)\n",
    "ax2.set_ylabel('Best k (n_neighbors)', fontsize=12)\n",
    "ax2.legend()\n",
    "ax2.grid(True, which='both', linestyle='--')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b08bed",
   "metadata": {},
   "source": [
    "While the KNN model demonstrated periods of significant predictive power, particularly in Fold 3, its performance was not robust or stable over the entire backtest period. The overall mean ROC AUC of approximately 0.5, combined with the high performance variance, indicates that this strategy would be unreliable in a live trading environment. The results suggest that while a predictive signal exists within the data, a simple pure vanilla KNN is likely insufficient to capture its non-stationary and complex nature consistently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbe5af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- 1. Define Model and Expanded Hyperparameter Grid ---\n",
    "knn_model = KNeighborsClassifier()\n",
    "param_grid = {\n",
    "    'n_neighbors': range(1, 10),\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['minkowski', 'manhattan']\n",
    "}\n",
    "\n",
    "\n",
    "# --- 2. Set up and Execute Walk-Forward Optimization ---\n",
    "n_splits = 4\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits, test_size=147)\n",
    "\n",
    "fold_results = []\n",
    "all_y_test_agg = []\n",
    "all_y_pred_proba_agg = []\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(tscv.split(X)):\n",
    "    fold_num = fold + 1\n",
    "    print(f\"\\n Processing Fold {fold_num}/{n_splits}\")\n",
    "\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    inner_tscv = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "    print(f\"Tuning KNN for Fold {fold_num}\")\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=knn_model, param_grid=param_grid, cv=inner_tscv, scoring='roc_auc', n_jobs=-1\n",
    "    )\n",
    "    grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "    best_params = grid_search.best_params_\n",
    "    best_knn_for_fold = grid_search.best_estimator_\n",
    "\n",
    "    y_pred_proba = best_knn_for_fold.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "    fold_results.append({\n",
    "        'Fold': fold_num,\n",
    "        'ROC AUC': roc_auc_score(y_test, y_pred_proba),\n",
    "        'Accuracy': accuracy_score(y_test, (y_pred_proba > 0.5).astype(int)),\n",
    "        'Best k': best_params['n_neighbors'],\n",
    "        'Best Weights': best_params['weights'],\n",
    "        'Best Metric': best_params['metric']\n",
    "    })\n",
    "\n",
    "    all_y_test_agg.extend(y_test)\n",
    "    all_y_pred_proba_agg.extend(y_pred_proba)\n",
    "\n",
    "    print(f\"  > Fold {fold_num} Test ROC AUC: {fold_results[-1]['ROC AUC']:.4f}\")\n",
    "    print(f\"  > Best Params Found: {best_params}\")\n",
    "\n",
    "# --- 3. Aggregate and Display Final Results ---\n",
    "\n",
    "# A) Overall Performance Metrics for all folds\n",
    "all_y_pred_agg = (np.array(all_y_pred_proba_agg) > 0.5).astype(int)\n",
    "print(classification_report(all_y_test_agg, all_y_pred_agg, target_names=['DOWN', 'UP']))\n",
    "\n",
    "# B) Fold-by-Fold Performance Breakdown\n",
    "results_df = pd.DataFrame(fold_results).set_index('Fold')\n",
    "print(results_df.round(4))\n",
    "\n",
    "# C) Final Summary Statistics\n",
    "mean_roc_auc_agg = roc_auc_score(all_y_test_agg, all_y_pred_proba_agg)\n",
    "print(f\"Aggregated Out-of-Sample ROC AUC: {mean_roc_auc_agg:.4f}\")\n",
    "print(f\"Mean of Fold ROC AUCs:            {results_df['ROC AUC'].mean():.4f}\")\n",
    "print(f\"Std Dev of Fold ROC AUCs:         {results_df['ROC AUC'].std():.4f}\")\n",
    "\n",
    "\n",
    "# --- 4. Visualize the Results ---\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "fig = plt.figure(figsize=(14, 10))\n",
    "\n",
    "# --- Plot 1: ROC Curve for Aggregated Predictions ---\n",
    "ax1 = plt.subplot2grid((2, 2), (0, 0))\n",
    "fpr, tpr, _ = roc_curve(all_y_test_agg, all_y_pred_proba_agg)\n",
    "ax1.plot(fpr, tpr, color='darkorange', lw=2, label=f'Aggregated ROC curve (AUC = {mean_roc_auc_agg:.4f})')\n",
    "ax1.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "ax1.set_xlim([0.0, 1.0])\n",
    "ax1.set_ylim([0.0, 1.05])\n",
    "ax1.set_xlabel('False Positive Rate')\n",
    "ax1.set_ylabel('True Positive Rate')\n",
    "ax1.set_title('Overall ROC Curve (All Folds)')\n",
    "ax1.legend(loc=\"lower right\")\n",
    "ax1.grid(True)\n",
    "\n",
    "# --- Plot 2: ROC AUC Score per Fold ---\n",
    "ax2 = plt.subplot2grid((2, 2), (0, 1))\n",
    "results_df['ROC AUC'].plot(kind='bar', ax=ax2, color='teal', edgecolor='black')\n",
    "ax2.axhline(0.5, ls='--', color='red', label='Random Guess (0.5)')\n",
    "ax2.set_title('Performance per Walk-Forward Fold')\n",
    "ax2.set_xlabel('Fold Number')\n",
    "ax2.set_ylabel('Out-of-Sample ROC AUC')\n",
    "ax2.legend()\n",
    "ax2.tick_params(axis='x', rotation=0)\n",
    "\n",
    "# --- Plot 3: Best 'k' Found per Fold ---\n",
    "ax3 = plt.subplot2grid((2, 2), (1, 0), colspan=2)\n",
    "results_df['Best k'].plot(kind='line', ax=ax3, marker='o', color='purple', label='Optimal k')\n",
    "results_df['Best k'].plot(kind='bar', ax=ax3, color='purple', alpha=0.3)\n",
    "ax3.set_title('Optimal \"k\" Found in Each Fold')\n",
    "ax3.set_xlabel('Fold Number')\n",
    "ax3.set_ylabel('Best k (n_neighbors)')\n",
    "ax3.legend()\n",
    "ax3.grid(True, which='both', linestyle='--')\n",
    "\n",
    "fig.suptitle('KNN Walk-Forward Optimization Analysis', fontsize=16, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bbdded",
   "metadata": {},
   "source": [
    "**Results and Analysis**\n",
    "To conduct a comprehensive evaluation of the K-Nearest Neighbors (KNN) model, I implemented a walk-forward optimization framework with enhanced hyperparameter tuning. In each of the four sequential folds, the model's key parameters: n_neighbors (k), weights, and distance metrics were optimized on the training data before performance was measured on the subsequent unseen test set. This robust methodology provides a realistic assessment of the model's predictive power and its stability over time.\n",
    "\n",
    "The analysis yielded an Aggregated Out-of-Sample ROC AUC of around 0.51 across all predictions. This score, being consistently near the 0.5 baseline of random chance, indicates that the model possesses a small but statistically persistent predictive edge. The low standard deviation of the fold ROC AUCs (0.0235) is a particularly important finding, suggesting that the model's performance is relatively stable and not subject to the wild swings seen in previous simpler tuning attempts.\n",
    "\n",
    "**Performance and Hyperparameter Adaptation Across Folds**\n",
    "The model achieved its strongest performance in Fold 1 suggesting the clearest patterns existed in the earlier data. Performance then showed a gradual, graceful decay over subsequent folds, finishing just above random chance in the final period. This trend is a classic sign of increasing market efficiency over time, where a once-viable edge slowly erodes.\n",
    "\n",
    "Overall Predictive Power: The aggregated classification report shows an overall accuracy of 52%. While the model's precision and recall are modest, they are balanced for both \"UP\" and \"DOWN\" classes, indicating the model is not merely guessing but is making informed, albeit slightly better-than-chance, predictions.\n",
    "\n",
    "**Conclusion**\n",
    "In summary, the extensively tuned KNN model demonstrates a consistent but small predictive edge. The graceful decay in performance over the walk-forward folds strongly suggests that the underlying market patterns are becoming less pronounced over time due to increasing efficiency. While the model's performance is not strong enough to be considered a standalone profitable strategy after accounting for transaction costs, it successfully proves that a quantifiable, non-random signal exists within our engineered features. This validates the feature engineering process and provides a solid foundation for testing more advanced model architectures that might capture this signal more effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe21204e",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
